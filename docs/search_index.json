[["index.html", "Data Wrangling Welcome How to read this book An evolving book", " Data Wrangling Sara Altman, Bill Behrman, Hadley Wickham 2021-09-10 Welcome How to read this book An evolving book This book is not intended to be static. Starting in January 2020, we use this book to teach data wrangling in the Stanford Data Challenge Lab (DCL) course. The DCL functions as a testing ground for educational materials, as our students give us routine feedback on what they read and do in the course. We use this feedback to constantly improve our materials, including this book. The source for the book is also available on GitHub where we welcome suggestions for improvements. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["manip-basics.html", "1 Basics 1.1 dplyr basics 1.2 Filter rows with filter() 1.3 Add new variables with mutate() 1.4 Grouped summaries with summarize() 1.5 Combining multiple operations with the pipe", " 1 Basics library(tidyverse) library(nycflights13) 1.1 dplyr basics In this section you will learn about three important dplyr functions that give you basic data manipulation power: Pick observations by their values (filter()). Create new variables from existing variables (mutate()). Collapse many values down to a single summary (summarize()). These can all be used in conjunction with group_by(), which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. You can use the Data Transformation cheatsheet to jog your memory, and learn about other dplyr functions we’ll cover in the future. All dplyr verbs work similarly: The first argument is a data frame. The subsequent arguments describe what to do with the data frame. The result is a new data frame. Together these properties make it easy to chain together multiple simple steps to achieve a complex result. Let’s dive in and see how these verbs work. 1.2 Filter rows with filter() filter() allows you to subset observations based on their values. The first argument is the name of the data frame. The second and subsequent arguments are the expressions that filter the data frame. For example, we can select all flights on January 1st with: flights %&gt;% filter(month == 1, day == 1) #&gt; # A tibble: 842 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 542 540 2 923 850 #&gt; 4 2013 1 1 544 545 -1 1004 1022 #&gt; 5 2013 1 1 554 600 -6 812 837 #&gt; 6 2013 1 1 554 558 -4 740 728 #&gt; # … with 836 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, #&gt; # flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, #&gt; # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; When you run that line of code, dplyr executes the filtering operation and returns a new data frame. dplyr functions never modify their inputs, so if you want to save the result, you’ll need to use the assignment operator, &lt;-: jan1 &lt;- flights %&gt;% filter(month == 1, day == 1) 1.2.1 Comparisons To use filtering effectively, you have to know how to select the observations that you want using the comparison operators. R provides the standard relational operators: &lt;, &gt;, &lt;=, &gt;=, == (equal), and != (not equal). When you’re starting out with R, the easiest mistake to make is to use = instead of == when testing for equality. When this happens, you’ll get an informative error: flights %&gt;% filter(month = 1) #&gt; Error: Problem with `filter()` input `..1`. #&gt; x Input `..1` is named. #&gt; ℹ This usually means that you&#39;ve used `=` instead of `==`. #&gt; ℹ Did you mean `month == 1`? 1.2.2 Logical operators Multiple arguments to filter() are combined with “and”: every expression must be true in order for a row to be included in the output. For other types of combinations, you’ll need to use Boolean operators yourself: &amp; is “and,” | is “or,” and ! is “not.” The following code finds all flights that departed in November or December: flights %&gt;% filter(month == 11 | month == 12) The order of operations doesn’t work like English. You can’t write filter(flights, month == 11 | 12), which you might literally translate into “finds all flights that departed in November or December.” Instead it finds all months that equal 11 | 12, an expression that evaluates to TRUE. In a numeric context (like here), TRUE becomes one, so this finds all flights in January, not November or December. This is quite confusing! A useful equivalent way to filter is to use x %in% y. This will select every row where x is one of the values in y. We could use it to rewrite the code above: nov_dec &lt;- flights %&gt;% filter(month %in% c(11, 12)) As well as &amp; and |, R also has &amp;&amp; and ||. Don’t use them here! You’ll learn more about them later. 1.3 Add new variables with mutate() Besides selecting sets of existing columns, it’s often useful to add new columns that are functions of existing columns. That’s the job of mutate(). mutate() always adds new columns at the end of your dataset, so we’ll start by creating a narrower dataset so we can see the new variables. Remember that when you’re in RStudio, the easiest way to see all the columns is view(). flights_small &lt;- flights %&gt;% select( year:day, ends_with(&quot;delay&quot;), distance, air_time ) flights_small %&gt;% mutate( gain = arr_delay - dep_delay, speed = 60 * distance / air_time ) #&gt; # A tibble: 336,776 × 9 #&gt; year month day dep_delay arr_delay distance air_time gain speed #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 2 11 1400 227 9 370. #&gt; 2 2013 1 1 4 20 1416 227 16 374. #&gt; 3 2013 1 1 2 33 1089 160 31 408. #&gt; 4 2013 1 1 -1 -18 1576 183 -17 517. #&gt; 5 2013 1 1 -6 -25 762 116 -19 394. #&gt; 6 2013 1 1 -4 12 719 150 16 288. #&gt; # … with 336,770 more rows Note that you can refer to columns that you’ve just created: flights_small %&gt;% mutate( gain = arr_delay - dep_delay, hours = air_time / 60, gain_per_hour = gain / hours ) #&gt; # A tibble: 336,776 × 10 #&gt; year month day dep_delay arr_delay distance air_time gain hours #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 2 11 1400 227 9 3.78 #&gt; 2 2013 1 1 4 20 1416 227 16 3.78 #&gt; 3 2013 1 1 2 33 1089 160 31 2.67 #&gt; 4 2013 1 1 -1 -18 1576 183 -17 3.05 #&gt; 5 2013 1 1 -6 -25 762 116 -19 1.93 #&gt; 6 2013 1 1 -4 12 719 150 16 2.5 #&gt; # … with 336,770 more rows, and 1 more variable: gain_per_hour &lt;dbl&gt; 1.4 Grouped summaries with summarize() The last key verb is summarize(). It collapses a data frame to a single row: flights %&gt;% summarize(delay = mean(dep_delay, na.rm = TRUE)) #&gt; # A tibble: 1 × 1 #&gt; delay #&gt; &lt;dbl&gt; #&gt; 1 12.6 (na.rm = TRUE removes the missing values so they don’t affect the final summary.) summarize() is not terribly useful unless we pair it with group_by(). This changes the unit of analysis from the complete dataset to individual groups. Then, when you use the dplyr verbs on a grouped data frame, they’ll be automatically applied “by group.” For example, if we applied exactly the same code to a data frame grouped by date, we get the average delay per date: by_day &lt;- flights %&gt;% group_by(year, month, day) by_day %&gt;% summarize(delay = mean(dep_delay, na.rm = TRUE)) #&gt; # A tibble: 365 × 4 #&gt; # Groups: year, month [12] #&gt; year month day delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 11.5 #&gt; 2 2013 1 2 13.9 #&gt; 3 2013 1 3 11.0 #&gt; 4 2013 1 4 8.95 #&gt; 5 2013 1 5 5.73 #&gt; 6 2013 1 6 7.15 #&gt; # … with 359 more rows Together group_by() and summarize() provide one of the tools that you’ll use most commonly when working with dplyr: grouped summaries. But before we go any further with this, we need to introduce a powerful new idea: the pipe. 1.5 Combining multiple operations with the pipe Imagine that we want to explore the relationship between the distance and average delay for each location. Using what you know about dplyr and not using pipes, you might write code like this: by_dest &lt;- group_by(flights, dest) delay &lt;- summarize( by_dest, count = n(), dist = mean(distance, na.rm = TRUE), delay = mean(arr_delay, na.rm = TRUE) ) delay &lt;- filter(delay, count &gt; 20, dest != &quot;HNL&quot;) ggplot(data = delay, mapping = aes(x = dist, y = delay)) + geom_point(aes(size = count), alpha = 1 / 3) + geom_smooth(se = FALSE) It looks like delays increase with distance up to ~750 miles and then decrease. Maybe as flights get longer there’s more ability to make up delays in the air? There are three steps to prepare this data: Group flights by destination. Summarize to compute distance, average delay, and number of flights. Filter to remove noisy points and Honolulu airport, which is almost twice as far away as the next closest airport. This code is a little frustrating to write because we have to give each intermediate data frame a name, even though we don’t care about it. Naming things is hard, so this slows down our analysis. It’s better to tackle the same problem with the pipe, %&gt;%: delays &lt;- flights %&gt;% group_by(dest) %&gt;% summarize( count = n(), dist = mean(distance, na.rm = TRUE), delay = mean(arr_delay, na.rm = TRUE) ) %&gt;% filter(count &gt; 20, dest != &quot;HNL&quot;) This focuses on the transformations, not what’s being transformed, which makes the code easier to read. You can read it as a series of imperative statements: group, then summarize, then filter. As suggested by this reading, a good way to pronounce %&gt;% when reading code is “then.” Behind the scenes, x %&gt;% f(y) turns into f(x, y), and x %&gt;% f(y) %&gt;% g(z) turns into g(f(x, y), z) and so on. You can use the pipe to rewrite multiple operations in a way that you can read left-to-right, top-to-bottom. We’ll use piping frequently from now on because it considerably improves the readability of code, and we’ll come back to it in more detail later on. "],["manip-one-table.html", "2 Other single-table verbs 2.1 Select 2.2 Rename 2.3 Change column order 2.4 Transmute 2.5 Arrange 2.6 Distinct 2.7 Slice rows 2.8 Sample", " 2 Other single-table verbs You’ve learned the most important verbs for data analysis: filter(), mutate(), group_by() and summarize(). There are a number of other verbs that are not quite as important but still come in handy from time-to-time. The goal of this section is to familiarize you with their purpose and basic operation. library(tidyverse) library(nycflights13) 2.1 Select Most of the datasets you’ll work with in this class only have a relatively small number of variables, and generally you don’t need to reduce further. In real life, you’ll sometimes encounter datasets with hundreds or even thousands of variables, and the first challenge is just to narrow down to a useful subset. Solving that problem is the job of select(). select() allows you to work with column names using a handful of helper functions: starts_with(\"x\") and ends_with(\"x\") select variables that start with a common prefix or end with a common suffix. contains(\"x\") selects variables that contain a phrase. matches(\"x.y\") select all variables that match a given regular expression (which you’ll learn about later in the course). a:e selects all variables from variable a to variable e inclusive. You can also select a single variable just by using its name directly. flights %&gt;% select(year:day, ends_with(&quot;delay&quot;)) #&gt; # A tibble: 336,776 × 5 #&gt; year month day dep_delay arr_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 2 11 #&gt; 2 2013 1 1 4 20 #&gt; 3 2013 1 1 2 33 #&gt; 4 2013 1 1 -1 -18 #&gt; 5 2013 1 1 -6 -25 #&gt; 6 2013 1 1 -4 12 #&gt; # … with 336,770 more rows To remove variables from selection, put an exclamation point ! in front of the expression. flights %&gt;% select(!starts_with(&quot;dep&quot;)) #&gt; # A tibble: 336,776 × 17 #&gt; year month day sched_dep_time arr_time sched_arr_time arr_delay carrier #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 2013 1 1 515 830 819 11 UA #&gt; 2 2013 1 1 529 850 830 20 UA #&gt; 3 2013 1 1 540 923 850 33 AA #&gt; 4 2013 1 1 545 1004 1022 -18 B6 #&gt; 5 2013 1 1 600 812 837 -25 DL #&gt; 6 2013 1 1 558 740 728 12 UA #&gt; # … with 336,770 more rows, and 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, #&gt; # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, #&gt; # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 2.2 Rename To change the name of a variable use df %&gt;% rename(new_name = old_name). If you have trouble remembering which sides old and new go on, remember it’s the same order as mutate(). flights %&gt;% rename(tail_num = tailnum) #&gt; # A tibble: 336,776 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 542 540 2 923 850 #&gt; 4 2013 1 1 544 545 -1 1004 1022 #&gt; 5 2013 1 1 554 600 -6 812 837 #&gt; 6 2013 1 1 554 558 -4 740 728 #&gt; # … with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tail_num &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; If you’re selecting and renaming, note that you can also use select() to rename. This sometimes allows you to save a step. flights %&gt;% select(year:day, tail_num = tailnum) #&gt; # A tibble: 336,776 × 4 #&gt; year month day tail_num #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 2013 1 1 N14228 #&gt; 2 2013 1 1 N24211 #&gt; 3 2013 1 1 N619AA #&gt; 4 2013 1 1 N804JB #&gt; 5 2013 1 1 N668DN #&gt; 6 2013 1 1 N39463 #&gt; # … with 336,770 more rows 2.3 Change column order Use relocate() to change column positions. It uses the same syntax as select(). The arguments .before and .after indicate where to move columns. If neither argument is supplied, columns are moved before the first column. flights %&gt;% relocate(carrier, flight, origin, dest, .after = dep_time) #&gt; # A tibble: 336,776 × 19 #&gt; year month day dep_time carrier flight origin dest sched_dep_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 UA 1545 EWR IAH 515 #&gt; 2 2013 1 1 533 UA 1714 LGA IAH 529 #&gt; 3 2013 1 1 542 AA 1141 JFK MIA 540 #&gt; 4 2013 1 1 544 B6 725 JFK BQN 545 #&gt; 5 2013 1 1 554 DL 461 LGA ATL 600 #&gt; 6 2013 1 1 554 UA 1696 EWR ORD 558 #&gt; # … with 336,770 more rows, and 10 more variables: dep_delay &lt;dbl&gt;, #&gt; # arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, tailnum &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 2.4 Transmute Transmute is a minor variation of mutate(). The main difference is that it drops any variables that you didn’t explicitly mention. It’s a useful shortcut for mutate() + select(). df &lt;- tibble(x = 1:3, y = 3:1) # mutate() keeps x and y df %&gt;% mutate(z = x + y) #&gt; # A tibble: 3 × 3 #&gt; x y z #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 3 4 #&gt; 2 2 2 4 #&gt; 3 3 1 4 # transmute() drops x and y df %&gt;% transmute(z = x + y) #&gt; # A tibble: 3 × 1 #&gt; z #&gt; &lt;int&gt; #&gt; 1 4 #&gt; 2 4 #&gt; 3 4 2.5 Arrange arrange() lets you change the order of the rows. To put a column in descending order, use desc(). flights %&gt;% arrange(desc(dep_delay)) #&gt; # A tibble: 336,776 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 9 641 900 1301 1242 1530 #&gt; 2 2013 6 15 1432 1935 1137 1607 2120 #&gt; 3 2013 1 10 1121 1635 1126 1239 1810 #&gt; 4 2013 9 20 1139 1845 1014 1457 2210 #&gt; 5 2013 7 22 845 1600 1005 1044 1815 #&gt; 6 2013 4 10 1100 1900 960 1342 2211 #&gt; # … with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; flights %&gt;% arrange(year, month, day) #&gt; # A tibble: 336,776 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 542 540 2 923 850 #&gt; 4 2013 1 1 544 545 -1 1004 1022 #&gt; 5 2013 1 1 554 600 -6 812 837 #&gt; 6 2013 1 1 554 558 -4 740 728 #&gt; # … with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 2.6 Distinct distinct() removes duplicates from a dataset. The result is ordered by first occurence in original dataset. flights %&gt;% distinct(carrier, flight) #&gt; # A tibble: 5,725 × 2 #&gt; carrier flight #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 UA 1545 #&gt; 2 UA 1714 #&gt; 3 AA 1141 #&gt; 4 B6 725 #&gt; 5 DL 461 #&gt; 6 UA 1696 #&gt; # … with 5,719 more rows 2.7 Slice rows slice() allows you to pick rows by position, by group. # Rows 10 - 14 flights %&gt;% slice(10:14) #&gt; # A tibble: 5 × 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 558 600 -2 753 745 #&gt; 2 2013 1 1 558 600 -2 849 851 #&gt; 3 2013 1 1 558 600 -2 853 856 #&gt; 4 2013 1 1 558 600 -2 924 917 #&gt; 5 2013 1 1 558 600 -2 923 937 #&gt; # … with 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, #&gt; # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; slice_head() and slice_tail() show the first (or last) rows of the data frame or its groups. # First 5 flights to each destination flights %&gt;% select(year:dep_time, carrier, flight, origin, dest) %&gt;% group_by(dest) %&gt;% slice_head(n = 5) #&gt; # A tibble: 517 × 8 #&gt; # Groups: dest [105] #&gt; year month day dep_time carrier flight origin dest #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2013 10 1 1955 B6 65 JFK ABQ #&gt; 2 2013 10 2 2010 B6 65 JFK ABQ #&gt; 3 2013 10 3 1955 B6 65 JFK ABQ #&gt; 4 2013 10 4 2017 B6 65 JFK ABQ #&gt; 5 2013 10 5 1959 B6 65 JFK ABQ #&gt; 6 2013 10 1 1149 B6 1191 JFK ACK #&gt; # … with 511 more rows # Last 5 flights overall flights %&gt;% select(year:dep_time, carrier, flight, origin, dest) %&gt;% slice_tail(n = 5) #&gt; # A tibble: 5 × 8 #&gt; year month day dep_time carrier flight origin dest #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2013 9 30 NA 9E 3393 JFK DCA #&gt; 2 2013 9 30 NA 9E 3525 LGA SYR #&gt; 3 2013 9 30 NA MQ 3461 LGA BNA #&gt; 4 2013 9 30 NA MQ 3572 LGA CLE #&gt; 5 2013 9 30 NA MQ 3531 LGA RDU slice_min() and slice_max() select rows by the highest or lowest values of a variable. By default all ties are returned, in which case you may receive more rows than you ask for. # Flights with the last time_hour flights %&gt;% select(time_hour, carrier, flight, origin, dest) %&gt;% slice_max(n = 1, time_hour) #&gt; # A tibble: 5 × 5 #&gt; time_hour carrier flight origin dest #&gt; &lt;dttm&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 2013-12-31 23:00:00 B6 839 JFK BQN #&gt; 2 2013-12-31 23:00:00 DL 412 JFK SJU #&gt; 3 2013-12-31 23:00:00 B6 1389 EWR SJU #&gt; 4 2013-12-31 23:00:00 B6 1503 JFK SJU #&gt; 5 2013-12-31 23:00:00 B6 745 JFK PSE 2.8 Sample When working with very large datasets, sometimes it’s convenient to reduce to a smaller dataset, just by taking a random sample. That’s the job of slice_sample(). You use the n argument to select the same number of observations from each group. You use the prop argument to select the same proportion. # Popular destinations popular_dest &lt;- flights %&gt;% group_by(dest) %&gt;% filter(n() &gt;= 1000) # Dataset with a random sample of 100 flights to each destination popular_dest %&gt;% slice_sample(n = 100) #&gt; # A tibble: 5,800 × 19 #&gt; # Groups: dest [58] #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 8 16 1543 1545 -2 1807 1806 #&gt; 2 2013 4 22 1537 1520 17 1835 1745 #&gt; 3 2013 10 29 1917 1930 -13 2152 2159 #&gt; 4 2013 11 14 1747 1729 18 2007 1955 #&gt; 5 2013 4 26 703 700 3 940 931 #&gt; 6 2013 12 17 NA 1240 NA NA 1513 #&gt; # … with 5,794 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # Dataset with a random sample of 1% of the flights to each destination popular_dest %&gt;% slice_sample(prop = 0.01) #&gt; # A tibble: 3,176 × 19 #&gt; # Groups: dest [58] #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 9 3 1955 1959 -4 2151 2230 #&gt; 2 2013 8 6 1924 1925 -1 2153 2156 #&gt; 3 2013 7 29 1256 1300 -4 1521 1519 #&gt; 4 2013 9 6 559 600 -1 811 815 #&gt; 5 2013 9 12 615 600 15 839 829 #&gt; 6 2013 6 3 1543 1545 -2 1844 1823 #&gt; # … with 3,170 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; "],["parse-basics.html", "3 Parsing 3.1 Delimited files 3.2 read_csv()", " 3 Parsing So far you’ve worked with data sets that have been bundled in R packages, or have been created with tibble() or tribble(). Now it’s time to learn how to read simple flat files from disk. To do this, we’ll use functions from readr. readr is one of the core tidyverse packages, so you won’t usually load it explicitly. library(tidyverse) 3.1 Delimited files In this section, we’re going to focus on delimited files. Delimited files have a delimiter between each value. Two types make up the majority of delimited files that you’ll see in the wild: CSV (comma separated) and TSV (tab separated). We’ll focus on CSV files, but everything you’ll learn applies equally to TSVs, replacing commas with tabs. A typical CSV file looks something like this: Sepal.Length,Sepal.Width,Petal.Length,Petal.Width,Species 5.1,3.5,1.4,0.2,setosa 4.9,3,1.4,0.2,setosa 4.7,3.2,1.3,0.2,setosa 4.6,3.1,1.5,0.2,setosa 5,3.6,1.4,0.2,setosa 5.4,3.9,1.7,0.4,setosa 4.6,3.4,1.4,0.3,setosa 5,3.4,1.5,0.2,setosa Note that: The first line gives the column names. Each subsequent line is one row of data. Each value is separated by a comma (hence the name). Typically, you can recognize a CSV file by its extension: .csv. But beware! Sometimes the extension lies, and if you’re getting weird errors when reading a file, it’s a good idea to peek inside the file using readr::read_lines() and writeLines(), specifying the n_max argument to just look at the first few lines. (You’ll learn more about writeLines() when we get to strings; for now just remember it’s a useful tool for printing lines to the screen.) &quot;data/heights.csv&quot; %&gt;% read_lines(n_max = 10) %&gt;% writeLines() #&gt; &quot;earn&quot;,&quot;height&quot;,&quot;sex&quot;,&quot;ed&quot;,&quot;age&quot;,&quot;race&quot; #&gt; 50000,74.4244387818035,&quot;male&quot;,16,45,&quot;white&quot; #&gt; 60000,65.5375428255647,&quot;female&quot;,16,58,&quot;white&quot; #&gt; 30000,63.6291977374349,&quot;female&quot;,16,29,&quot;white&quot; #&gt; 50000,63.1085616752971,&quot;female&quot;,16,91,&quot;other&quot; #&gt; 51000,63.4024835710879,&quot;female&quot;,17,39,&quot;white&quot; #&gt; 9000,64.3995075440034,&quot;female&quot;,15,26,&quot;white&quot; #&gt; 29000,61.6563258264214,&quot;female&quot;,12,49,&quot;white&quot; #&gt; 32000,72.6985437364783,&quot;male&quot;,17,46,&quot;white&quot; #&gt; 2000,72.0394668497611,&quot;male&quot;,15,21,&quot;hispanic&quot; This file illustrates another feature present in many CSV files: some values are surrounded by quotes. Confusingly, this isn’t a guarantee that the value is a string: some CSV files also surround numbers in quotes too. As you work with more CSV files, you’ll discover there are few hard and fast rules: for pretty much every crazy thing that you can imagine, someone has done it in a CSV file somewhere. 3.2 read_csv() The workhorse for reading in CSV files is called read_csv(). You give it a path to a CSV file and it gives you back a tibble: heights &lt;- read_csv(&quot;data/heights.csv&quot;) #&gt; Rows: 1192 Columns: 6 #&gt; ── Column specification ──────────────────────────────────────────────────────── #&gt; Delimiter: &quot;,&quot; #&gt; chr (2): sex, race #&gt; dbl (4): earn, height, ed, age #&gt; #&gt; ℹ Use `spec()` to retrieve the full column specification for this data. #&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. heights #&gt; # A tibble: 1,192 × 6 #&gt; earn height sex ed age race #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 50000 74.4 male 16 45 white #&gt; 2 60000 65.5 female 16 58 white #&gt; 3 30000 63.6 female 16 29 white #&gt; 4 50000 63.1 female 16 91 other #&gt; 5 51000 63.4 female 17 39 white #&gt; 6 9000 64.4 female 15 26 white #&gt; # … with 1,186 more rows If you are very lucky, you can point read_csv() at a file and it just works. But this is usually the exception, not the rule, and often you’ll need to tweak some arguments. The most important arguments to read_csv() are: col_names: Usually col_names = TRUE, the default, which tells read_csv() that the first line of the file contains the column names. If there aren’t any column, names set col_names = FALSE or supply a character vector telling read_csv() what they should be col_names = c(\"x\", \"y\", \"z\"). col_types: You might have noticed that when we called read_csv() above it printed out a list of column “specifications.” That describes how readr converts each column into an data structure. readr uses some pretty good heuristics to guess the type, but sometimes the heuristics fail and you’ll need to supply the truth. You’ll learn more about that later in the course. It’s fairly common to encounter CSV files that have a bunch of lines at the top you don’t wish to parse. You can use skip = n to skip the first n lines, or comment = \"#\" to ignore all lines that start with #. read_csv() expects missing values to be supplied as \"\" or \"NA\". If your file uses a different convention, use the na argument to override the default. You’ll get to practice using these arguments in the exercises. "],["tidy.html", "4 Introduction", " 4 Introduction The idea of tidy data is central to the tidyverse. If you’re not already familiar with the features of tidy data, we recommend the following sections from Chapter 12 of R for Data Science: 12.1: Introduction 12.2: Tidy Data Most of the data you’ll encounter won’t be tidy. In this section, we’ll introduce some key tools for tidying data: the pivot functions. "],["pivot-basic.html", "5 Basic pivoting 5.1 Longer 5.2 Wider 5.3 Missing values", " 5 Basic pivoting library(tidyverse) library(dcldata) Most of the data you’ll encounter won’t be tidy, and it will be your job to figure out how to make it tidy. In this chapter, you’ll learn about two of the most important tidying tools: pivot_longer() and pivot_wider(). First, recall the characteristics of tidy data: Each value has its own cell. Each variable has its own column. Each observation has its own row. Non-tidy data will not fulfill one or more of these characteristics. 5.1 Longer example_eagle_nests contains data on the number of bald eagle nesting sites across multiple regions and years. # Source: US Fish and Wildlife Service example_eagle_nests #&gt; # A tibble: 3 × 3 #&gt; region `2007` `2009` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Pacific 1039 2587 #&gt; 2 Southwest 51 176 #&gt; 3 Rocky Mountains and Plains 200 338 The data isn’t tidy. In the following steps, we’ll show you how to tidy example_eagle_nests using pivot_longer(). Step 1: Identify the variables. There are three variables in this dataset: region: The US region where the nests are located. year: The year the nests were found. num_nests: The number of nests found. Only one of these variables (region) is currently a column. Values of year are currently stored horizontally as column names, and values of num_nests are stored as values of 2007 and 2009. In order for this data to be tidy, we’ll need to pivot 2007 and 2009 into a year column, and the values of 2007 and 2009 into a num_nests column. Now that we’ve identified the variables, we can start filling in our call to pivot_longer(). We’ll need three arguments, which we’ll identify over the next three steps. example_eagle_nests %&gt;% pivot_longer( # Step 2 # Step 3 # Step 4 ) Step 2: Identify the columns to pivot. To decide which columns to pivot, identify which columns are keeping the data from being tidy. In our example, those columns are 2007 and 2009. 2007 and 2009 are actually values of year, not variables themselves, and their values are actually values of num_nests. The cols argument controls which columns pivot_longer() pivots. example_eagle_nests %&gt;% pivot_longer( cols = c(`2007`, `2009`), # Step 3 # Step 4 ) cols is similar to select(). You can specify columns by name, with contains(), starts_with(), etc. Here, we have to wrap 2007 and 2009 in backticks (` `) because they start with numbers. Step 3: Name the column that will store the values from the column names. Now, we’re just going to focus on the columns we identified in cols. Ultimately, pivot_longer() is going to move both the names of these columns and their values into new, separate columns. First, we’ll focus on the column names: 2007 and 2009. pivot_longer()’s names_to argument controls the name of the column that will store the old column names. We want to name this new column \"year\". The argument is called names_to because you’re specifying which column to move the column names to. example_eagle_nests %&gt;% pivot_longer( cols = c(`2007`, `2009`), names_to = &quot;year&quot;, # Step 4 ) Note that the argument to names_to has to be in quotes, while the arguments to cols do not. It’s easy to get confused about which pivot arguments need to be quoted. Here’s the general rule: if you’re identifying an existing column (e.g., 2007), do not quote. If you’re talking about a column that does not currently exist (e.g., year), quote it. Now, pivot_longer() will create a new column called year and fill it with the column names 2007 and 2009. Because we specified two columns in cols, we will get two values of year for each region. Step 4: Name the column that will store the column values. Now, we need to name the column that will store the values from 2007 and 2009. Just as names_to controls the name of the column for the names, values_to controls the name of column for the values. In example_eagle_nests, the column values represent the number of nests, so we’ll name the new column \"num_nests\". example_eagle_nests %&gt;% pivot_longer( cols = c(`2007`, `2009`), names_to = &quot;year&quot;, values_to = &quot;num_nests&quot; ) Again, notice that you have to quote any argument to values_to because it references a column that does not exist. pivot_longer() will now move the values from 2007 and 2009 to a column called num_nests. Here’s the function call again with the results. example_eagle_nests %&gt;% pivot_longer( cols = c(`2007`, `2009`), names_to = &quot;year&quot;, values_to = &quot;num_nests&quot; ) #&gt; # A tibble: 6 × 3 #&gt; region year num_nests #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Pacific 2007 1039 #&gt; 2 Pacific 2009 2587 #&gt; 3 Southwest 2007 51 #&gt; 4 Southwest 2009 176 #&gt; 5 Rocky Mountains and Plains 2007 200 #&gt; 6 Rocky Mountains and Plains 2009 338 The data is now tidy! pivot_longer() has many optional arguments, but cols, names_to, and values_to will cover most of your use-cases. The Missing values section below and the Advanced pivoting chapter cover some more specialized uses of pivot_longer(). Here’s another eagle-related example. example_eagle_pairs contains data on the number of observed bald eagle breeding pairs across years and states. example_eagle_pairs #&gt; # A tibble: 48 × 12 #&gt; state state_abbr `1997` `1998` `1999` `2000` `2001` `2002` `2003` `2004` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Alabama AL 22 23 26 27 NA NA 47 NA #&gt; 2 Arizona AZ 34 36 38 37 37 43 43 NA #&gt; 3 Arkansas AR 24 29 34 36 NA NA 36 42 #&gt; 4 California CA 142 148 151 NA NA NA 160 NA #&gt; 5 Colorado CO 29 27 29 42 45 NA NA NA #&gt; 6 Connecticut CT 2 2 2 4 6 8 8 NA #&gt; # … with 42 more rows, and 2 more variables: 2005 &lt;int&gt;, 2006 &lt;int&gt; Again, the data isn’t tidy because values are spread across column names. We need to pivot all the year columns (1997 through 2006), moving their names into a column named \"year\" and their values into a column named \"num_nests\". Here’s the full call to pivot_longer(): example_eagle_pairs %&gt;% pivot_longer( cols = !starts_with(&quot;state&quot;), names_to = &quot;year&quot;, values_to = &quot;num_pairs&quot; ) #&gt; # A tibble: 480 × 4 #&gt; state state_abbr year num_pairs #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Alabama AL 1997 22 #&gt; 2 Alabama AL 1998 23 #&gt; 3 Alabama AL 1999 26 #&gt; 4 Alabama AL 2000 27 #&gt; 5 Alabama AL 2001 NA #&gt; 6 Alabama AL 2002 NA #&gt; # … with 474 more rows 5.2 Wider pivot_wider() is the inverse of pivot_longer(). pivot_longer() moves data from column names to cell values, while pivot_wider() pulls data from cell values into column names, creating a wider tibble. You’ll likely use pivot_longer() more often than pivot_wider() when tidying. Often, you’ll actually use pivot_wider() to un-tidy data. The non-tidy format may be more convenient for some tasks (e.g., creating a specific visualization). To explain pivot_wider(), we’ll turn the tidied example_eagle_nests back into its original form. Here’s the tidied version: example_eagle_nests_tidy #&gt; # A tibble: 6 × 3 #&gt; region year num_nests #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Pacific 2007 1039 #&gt; 2 Pacific 2009 2587 #&gt; 3 Southwest 2007 51 #&gt; 4 Southwest 2009 176 #&gt; 5 Rocky Mountains and Plains 2007 200 #&gt; 6 Rocky Mountains and Plains 2009 338 Step 1 Identify the column whose values will supply the column names. pivot_wider() turns the values from one column and turns them into column names. In our example, we want the unique values from year to become column names. pivot_wider()’s names_from argument controls which column is pivoted into column names. example_eagle_nests_tidy %&gt;% pivot_wider( names_from = year, # Step 2 ) Notice that year is unquoted because, following the rule, year does exist in example_eagle_nests_tidy. Step 2 Identify the column whose values will supply the column values. Now, we need to identify the column that will supply the values of 2007 and 2009. In example_eagle_nests_tidy, that’s num_nests. We specify num_nests as the values_from argument. example_eagle_nests_tidy %&gt;% pivot_wider( names_from = year, values_from = num_nests ) #&gt; # A tibble: 3 × 3 #&gt; region `2007` `2009` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Pacific 1039 2587 #&gt; 2 Southwest 51 176 #&gt; 3 Rocky Mountains and Plains 200 338 Again, supply the name of the column unquoted. We’re done! The tibble is now transformed back into its original form. Let’s see an example of a tibble that actually does need pivot_wider() to be tidy. example_acs_1 contains data from the 2013-2017 American Community Survey, obtained through the tidycensus package. example_acs_1 #&gt; # A tibble: 156 × 4 #&gt; geoid name variable estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 01 Alabama pop_housed 4731852 #&gt; 2 01 Alabama pop_renter 1434765 #&gt; 3 01 Alabama median_rent 747 #&gt; 4 02 Alaska pop_housed 710743 #&gt; 5 02 Alaska pop_renter 241484 #&gt; 6 02 Alaska median_rent 1200 #&gt; # … with 150 more rows variable and estimate are not really variables (if you see a variable named variable it’s a good sign you need pivot_wider()). There are three distinct values in variable: example_acs_1 %&gt;% distinct(variable) #&gt; # A tibble: 3 × 1 #&gt; variable #&gt; &lt;chr&gt; #&gt; 1 pop_housed #&gt; 2 pop_renter #&gt; 3 median_rent Each of these values is actually a variable whose values are currently stored in estimate. To pivot, we’ll set names_from to variable and values_from to estimate. example_acs_1 %&gt;% pivot_wider(names_from = variable, values_from = estimate) #&gt; # A tibble: 52 × 5 #&gt; geoid name pop_housed pop_renter median_rent #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 01 Alabama 4731852 1434765 747 #&gt; 2 02 Alaska 710743 241484 1200 #&gt; 3 04 Arizona 6656124 2460534 972 #&gt; 4 05 Arkansas 2894098 965690 709 #&gt; 5 06 California 38168482 17066023 1358 #&gt; 6 08 Colorado 5318396 1782975 1125 #&gt; # … with 46 more rows 5.3 Missing values The United Nations compiles data on the origin and destination countries of international migrants. example_migration contains a subset of this data from 2017. The countries in the column names represent countries of origin, and the countries in dest represent destination countries. example_migration #&gt; # A tibble: 3 × 6 #&gt; dest Afghanistan Canada India Japan `South Africa` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Albania &lt;NA&gt; 913 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 2 Bulgaria 483 713 281 213 260 #&gt; 3 Romania &lt;NA&gt; &lt;NA&gt; 102 &lt;NA&gt; &lt;NA&gt; Again, the data isn’t tidy. Afghanistan, Canada, etc. are values of a variable, not variables themselves. We can use pivot_longer() to tidy the data. example_migration %&gt;% pivot_longer(cols = !dest, names_to = &quot;origin&quot;, values_to = &quot;migrants&quot;) #&gt; # A tibble: 15 × 3 #&gt; dest origin migrants #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Albania Afghanistan &lt;NA&gt; #&gt; 2 Albania Canada 913 #&gt; 3 Albania India &lt;NA&gt; #&gt; 4 Albania Japan &lt;NA&gt; #&gt; 5 Albania South Africa &lt;NA&gt; #&gt; 6 Bulgaria Afghanistan 483 #&gt; # … with 9 more rows There are a lot of NAs in the data. However, they don’t actually represent missing values. Someone didn’t forget to measure the number of migrants Afghanistan to Albania—there just weren’t any. It doesn’t really make sense to include these rows in our new, tidied dataset. We can use values_drop_na to exclude these rows. example_migration %&gt;% pivot_longer( cols = !dest, names_to = &quot;origin&quot;, values_to = &quot;migrants&quot;, values_drop_na = TRUE ) #&gt; # A tibble: 7 × 3 #&gt; dest origin migrants #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Albania Canada 913 #&gt; 2 Bulgaria Afghanistan 483 #&gt; 3 Bulgaria Canada 713 #&gt; 4 Bulgaria India 281 #&gt; 5 Bulgaria Japan 213 #&gt; 6 Bulgaria South Africa 260 #&gt; # … with 1 more row When you use values_drop_na = TRUE in pivot_longer(), you’re turning explicit missing values into implicit missing values. This is only a good idea if the NAs were in the non-tidy data for a purely structural reason, like in example_migration. In contrast, example_eagle_pairs’s NAs aren’t structural and represent actual missing data. Paired Alabamian eagles probably existed in 2001, but the data isn’t there. example_eagle_pairs #&gt; # A tibble: 48 × 12 #&gt; state state_abbr `1997` `1998` `1999` `2000` `2001` `2002` `2003` `2004` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Alabama AL 22 23 26 27 NA NA 47 NA #&gt; 2 Arizona AZ 34 36 38 37 37 43 43 NA #&gt; 3 Arkansas AR 24 29 34 36 NA NA 36 42 #&gt; 4 California CA 142 148 151 NA NA NA 160 NA #&gt; 5 Colorado CO 29 27 29 42 45 NA NA NA #&gt; 6 Connecticut CT 2 2 2 4 6 8 8 NA #&gt; # … with 42 more rows, and 2 more variables: 2005 &lt;int&gt;, 2006 &lt;int&gt; If we used values_drop_na = TRUE when we pivoted example_eagle_pairs, we would turn all these explicit missing values implicit, which isn’t a good idea. "],["pivot-advanced.html", "6 Advanced pivoting 6.1 Longer 6.2 Wider", " 6 Advanced pivoting pivot_longer() and pivot_wider() are very flexible, and can easily tidy a wide variety of non-tidy datasets. The previous chapter only covered the basics. In this chapter, we’ll explore this flexibility by introducing some of the pivot functions’ advanced functionality. 6.1 Longer 6.1.1 Types By default, pivot_longer() creates the names_to column as a character variable. For example, when we pivot_longer() example_eagle_nests, year becomes a character column. example_eagle_nests %&gt;% pivot_longer( cols = c(`2007`, `2009`), names_to = &quot;year&quot;, values_to = &quot;num_nests&quot; ) #&gt; # A tibble: 6 × 3 #&gt; region year num_nests #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Pacific 2007 1039 #&gt; 2 Pacific 2009 2587 #&gt; 3 Southwest 2007 51 #&gt; 4 Southwest 2009 176 #&gt; 5 Rocky Mountains and Plains 2007 200 #&gt; 6 Rocky Mountains and Plains 2009 338 It’s probably more useful to store year as an integer. We can tell pivot_longer() our desired type for the names_to column by using the optional names_transform argument. names_transform takes a named list of column name and function pairs. For example, here’s how we would create year as an integer column: example_eagle_nests %&gt;% pivot_longer( cols = c(`2007`, `2009`), names_to = &quot;year&quot;, names_transform = list(year = as.integer), values_to = &quot;num_nests&quot; ) #&gt; # A tibble: 6 × 3 #&gt; region year num_nests #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Pacific 2007 1039 #&gt; 2 Pacific 2009 2587 #&gt; 3 Southwest 2007 51 #&gt; 4 Southwest 2009 176 #&gt; 5 Rocky Mountains and Plains 2007 200 #&gt; 6 Rocky Mountains and Plains 2009 338 If we wanted year to be a double, we would use the following: example_eagle_nests %&gt;% pivot_longer( cols = c(`2007`, `2009`), names_to = &quot;year&quot;, names_transform = list(year = as.double), values_to = &quot;num_nests&quot; ) #&gt; # A tibble: 6 × 3 #&gt; region year num_nests #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Pacific 2007 1039 #&gt; 2 Pacific 2009 2587 #&gt; 3 Southwest 2007 51 #&gt; 4 Southwest 2009 176 #&gt; 5 Rocky Mountains and Plains 2007 200 #&gt; 6 Rocky Mountains and Plains 2009 338 pivot_longer() also has a values_transform argument that controls the type of the values_to column. You specify values_transform in the same way as names_transform. For example, say we wanted to change num_nests from its default type (double) to an integer. example_eagle_nests %&gt;% pivot_longer( cols = c(`2007`, `2009`), names_to = &quot;year&quot;, names_transform = list(year = as.integer), values_to = &quot;num_nests&quot;, values_transform = list(num_nests = as.integer) ) #&gt; # A tibble: 6 × 3 #&gt; region year num_nests #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Pacific 2007 1039 #&gt; 2 Pacific 2009 2587 #&gt; 3 Southwest 2007 51 #&gt; 4 Southwest 2009 176 #&gt; 5 Rocky Mountains and Plains 2007 200 #&gt; 6 Rocky Mountains and Plains 2009 338 6.1.2 Prefixes Sometimes, the columns you want to pivot will contain extra information, either in their names or their values. For example, example_gymnastics_1 contains data on the scores of three countries’ women’s Olympic gymnastic teams in 2016. example_gymnastics_1 #&gt; # A tibble: 3 × 3 #&gt; country score_vault score_floor #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 United States 46.9 46.0 #&gt; 2 Russia 45.7 42.0 #&gt; 3 China 44.3 42.1 To tidy example_gymnastics_1, we need to pivot score_vault and score_floor. example_gymnastics_1 %&gt;% pivot_longer( cols = !country, names_to = &quot;event&quot;, values_to = &quot;score&quot; ) #&gt; # A tibble: 6 × 3 #&gt; country event score #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 United States score_vault 46.9 #&gt; 2 United States score_floor 46.0 #&gt; 3 Russia score_vault 45.7 #&gt; 4 Russia score_floor 42.0 #&gt; 5 China score_vault 44.3 #&gt; 6 China score_floor 42.1 However, score_vault isn’t really a value of event. It would be better for the values to be \"vault\" and \"floor\". We can remove the \"score_\" prefix with pivot_longer()'s names_prefix argument. example_gymnastics_1 %&gt;% pivot_longer( cols = !country, names_to = &quot;event&quot;, names_prefix = &quot;score_&quot;, values_to = &quot;score&quot; ) #&gt; # A tibble: 6 × 3 #&gt; country event score #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 United States vault 46.9 #&gt; 2 United States floor 46.0 #&gt; 3 Russia vault 45.7 #&gt; 4 Russia floor 42.0 #&gt; 5 China vault 44.3 #&gt; 6 China floor 42.1 6.1.3 Multiple values example_gymnastics_2 includes data on both the 2012 and 2016 Olympics. example_gymnastics_2 #&gt; # A tibble: 3 × 5 #&gt; country vault_2012 floor_2012 vault_2016 floor_2016 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 United States 48.1 45.4 46.9 46.0 #&gt; 2 Russia 46.4 41.6 45.7 42.0 #&gt; 3 China 44.3 40.8 44.3 42.1 There are four variables in this non-tidy dataset: country, event, year, and score. To tidy the data, we’ll need to pivot vault_2012 through floor_2016. In contrast to other examples we’ve seen so far, though, each of these variable names contains two values: a event value and a year value. If we pivot_longer() as usual, both of these values will get placed in the same cell: example_gymnastics_2 %&gt;% pivot_longer( cols = !country, names_to = &quot;event_year&quot;, values_to = &quot;score&quot; ) #&gt; # A tibble: 12 × 3 #&gt; country event_year score #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 United States vault_2012 48.1 #&gt; 2 United States floor_2012 45.4 #&gt; 3 United States vault_2016 46.9 #&gt; 4 United States floor_2016 46.0 #&gt; 5 Russia vault_2012 46.4 #&gt; 6 Russia floor_2012 41.6 #&gt; # … with 6 more rows event_year contains two variables, and so the result isn’t tidy. Instead, we want pivot_longer() to split each pivoted variable name into two, placing \"vault\" or \"floor\" into an event variable and 2012 or 2016 into a year variable. We’ll need to make the following two changes to our pivot_longer() call: Change names_to to be a vector of two names: c(\"event\", \"year). This will tell pivot_longer() to create two variables from the column names instead of one. Use the names_sep argument to tell pivot_longer() what separates an event value from a year in each of the column names. Here, that’s an underscore (_). example_gymnastics_2 %&gt;% pivot_longer( cols = !country, names_to = c(&quot;event&quot;, &quot;year&quot;), names_sep = &quot;_&quot;, values_to = &quot;score&quot; ) #&gt; # A tibble: 12 × 4 #&gt; country event year score #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 United States vault 2012 48.1 #&gt; 2 United States floor 2012 45.4 #&gt; 3 United States vault 2016 46.9 #&gt; 4 United States floor 2016 46.0 #&gt; 5 Russia vault 2012 46.4 #&gt; 6 Russia floor 2012 41.6 #&gt; # … with 6 more rows pivot_longer() created three variables, instead of the default two: event, year, and score. Now, the data is tidy. We can extend this idea to work with any number of columns. example_gymnastics_3 has three values stored in each column name: event, year, and gender. example_gymnastics_3 #&gt; # A tibble: 3 × 9 #&gt; country vault_2012_f vault_2012_m vault_2016_f vault_2016_m floor_2012_f #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 United States 48.1 46.6 46.9 45.9 45.4 #&gt; 2 Russia 46.4 46.9 45.7 46.0 41.6 #&gt; 3 China 44.3 48.3 44.3 45 40.8 #&gt; # … with 3 more variables: floor_2012_m &lt;dbl&gt;, floor_2016_f &lt;dbl&gt;, #&gt; # floor_2016_m &lt;dbl&gt; names_sep is still _, but we’ll need to change names_to to include \"gender\". example_gymnastics_3 %&gt;% pivot_longer( cols = !country, names_to = c(&quot;event&quot;, &quot;year&quot;, &quot;gender&quot;), names_sep = &quot;_&quot;, values_to = &quot;score&quot; ) #&gt; # A tibble: 24 × 5 #&gt; country event year gender score #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 United States vault 2012 f 48.1 #&gt; 2 United States vault 2012 m 46.6 #&gt; 3 United States vault 2016 f 46.9 #&gt; 4 United States vault 2016 m 45.9 #&gt; 5 United States floor 2012 f 45.4 #&gt; 6 United States floor 2012 m 45.3 #&gt; # … with 18 more rows Finally, what if there isn’t a separator like _ in the column names? example_gymnastics_4 #&gt; # A tibble: 3 × 5 #&gt; country vault2012 floor2012 vault2016 floor2016 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 United States 48.1 45.4 46.9 46.0 #&gt; 2 Russia 46.4 41.6 45.7 42.0 #&gt; 3 China 44.3 40.8 44.3 42.1 Instead of using names_sep, we can a related pivot_longer() argument: names_pattern. names_pattern is more flexible than names_sep because it allows regular expression groups, matching each group with a variable from names_to. For example_gymnastics_4, we want to create two variables from the column names: event and year. This means names_pattern needs two groups. Here’s a regular expression we could use: &quot;([A-Za-z]+)(\\\\d+)&quot; \"([A-Za-z]+)\" matches only letters, so will pull out just \"vault\" or \"floor\". \"(\\\\d+)\" matches digits and will pull out 2012 or 2016. Here’s the call to pivot_longer(): example_gymnastics_4 %&gt;% pivot_longer( cols = !country, names_to = c(&quot;event&quot;, &quot;year&quot;), names_pattern = &quot;([A-Za-z]+)(\\\\d+)&quot;, values_to = &quot;score&quot; ) #&gt; # A tibble: 12 × 4 #&gt; country event year score #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 United States vault 2012 48.1 #&gt; 2 United States floor 2012 45.4 #&gt; 3 United States vault 2016 46.9 #&gt; 4 United States floor 2016 46.0 #&gt; 5 Russia vault 2012 46.4 #&gt; 6 Russia floor 2012 41.6 #&gt; # … with 6 more rows pivot_longer() successfully separates the events from the years. 6.2 Wider 6.2.1 Prefixes Like pivot_longer(), pivot_wider() also has a names_prefix argument. However, it adds a prefix instead of removing one. example_twins contains some data on two sets of (real) twins. n defines the birth order. example_twins #&gt; # A tibble: 4 × 3 #&gt; family name n #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Kelly Mark 1 #&gt; 2 Kelly Scott 2 #&gt; 3 Quin Tegan 1 #&gt; 4 Quin Sara 2 If we pivot_wider() without a prefix, we’ll get numbers as column names, which isn’t very informative. example_twins %&gt;% pivot_wider(names_from = &quot;n&quot;, values_from = &quot;name&quot;) #&gt; # A tibble: 2 × 3 #&gt; family `1` `2` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Kelly Mark Scott #&gt; 2 Quin Tegan Sara We can use names_prefix to add an informative prefix. example_twins %&gt;% pivot_wider( names_from = &quot;n&quot;, names_prefix = &quot;twin_&quot;, values_from = &quot;name&quot; ) #&gt; # A tibble: 2 × 3 #&gt; family twin_1 twin_2 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Kelly Mark Scott #&gt; 2 Quin Tegan Sara 6.2.2 Multiple values Earlier, we showed how you can create multiple columns from data stored in column names using pivot_longer(). Analogously, you can use pivot_wider() to create column names that combine values from multiple columns. For example, take the following modified subset of the American Community Survey data from last chapter: example_acs_2 #&gt; # A tibble: 8 × 5 #&gt; geoid name variable measure value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 01 Alabama pop_renter estimate 1434765 #&gt; 2 01 Alabama pop_renter error 16736 #&gt; 3 01 Alabama median_rent estimate 747 #&gt; 4 01 Alabama median_rent error 3 #&gt; 5 13 Georgia pop_renter estimate 3592422 #&gt; 6 13 Georgia pop_renter error 33385 #&gt; # … with 2 more rows The measure column indicates if a given row represents the estimate of the variable or the margin of error. We might want each combination of variable and measure to become the name of a new variable (i.e., pop_renter_estimate, pop_renter_error, median_rent_estimate, median_rent_error). Recall that pivot_wider()’s names_from argument controls which column’s values are used for the new column names. If we supply multiple columns to names_from, pivot_wider() will create one new column for each unique combination. example_acs_2 %&gt;% pivot_wider( names_from = c(variable, measure), values_from = value ) #&gt; # A tibble: 2 × 6 #&gt; geoid name pop_renter_estimate pop_renter_error median_rent_estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 01 Alabama 1434765 16736 747 #&gt; 2 13 Georgia 3592422 33385 927 #&gt; # … with 1 more variable: median_rent_error &lt;dbl&gt; By default, pivot_wider() will combine the two values with an underscore, but you can control the separator with names_sep. In example_acs_2, the names of our desired columns were stored across two variables: variable and measure. You might also encounter values stored across multiple variables. example_acs_3 #&gt; # A tibble: 4 × 5 #&gt; geoid name variable estimate error #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 01 Alabama pop_renter 1434765 16736 #&gt; 2 01 Alabama median_rent 747 3 #&gt; 3 13 Georgia pop_renter 3592422 33385 #&gt; 4 13 Georgia median_rent 927 3 Now, values of our desired variables (pop_renter_esimate, pop_renter_error, median_rent_estimate, and median_rent_error) are in two separate columns: estimate and error. We can specify both columns to pivot_wider()’s values_from argument. example_acs_3 %&gt;% pivot_wider( names_from = variable, values_from = c(estimate, error) ) #&gt; # A tibble: 2 × 6 #&gt; geoid name estimate_pop_renter estimate_median_rent error_pop_renter #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 01 Alabama 1434765 747 16736 #&gt; 2 13 Georgia 3592422 927 33385 #&gt; # … with 1 more variable: error_median_rent &lt;dbl&gt; pivot_wider() pivots variable, then creates columns by combining the value of measure with each column name specified in values_from. 6.2.3 id_cols pivot_wider() has an additional argument id_cols, which is useful in some situations. To explain, we’ll go over a brief example. Here’s another ACS dataset: example_acs_4 #&gt; # A tibble: 6 × 5 #&gt; geoid name variable estimate error #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 01 Alabama pop_housed 4731852 NA #&gt; 2 01 Alabama pop_renter 1434765 16736 #&gt; 3 01 Alabama median_rent 747 3 #&gt; 4 13 Georgia pop_housed 9943137 NA #&gt; 5 13 Georgia pop_renter 3592422 33385 #&gt; 6 13 Georgia median_rent 927 3 Now, variable includes three variables: the total housed population (\"pop_housed\"), total renter population (\"pop_renter\"), and median rent (\"median_rent\"). However, error is only available for \"pop_renter\" and \"median_rent\". Say that we don’t want to pivot error because we don’t need the margin of error data. Here’s an attempt: example_acs_4 %&gt;% pivot_wider( names_from = variable, values_from = estimate ) #&gt; # A tibble: 6 × 6 #&gt; geoid name error pop_housed pop_renter median_rent #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 01 Alabama NA 4731852 NA NA #&gt; 2 01 Alabama 16736 NA 1434765 NA #&gt; 3 01 Alabama 3 NA NA 747 #&gt; 4 13 Georgia NA 9943137 NA NA #&gt; 5 13 Georgia 33385 NA 3592422 NA #&gt; 6 13 Georgia 3 NA NA 927 This probably isn’t the result we want. pivot_wider() created one row for each geoid-name-error combination, thinking that geoid, name, and error all identify an observation. We can fix this by setting id_cols. id_cols controls which columns define an observation. By default, id_cols includes all columns not specified in names_from or values_from. Here, we want pivot_wider() to understand that each observation should be a state, defined by geoid and name. To fix the problem, we’ll tell pivot_longer() that error should not be included in id_cols. example_acs_4 %&gt;% pivot_wider( id_cols = !error, names_from = variable, values_from = estimate ) #&gt; # A tibble: 2 × 5 #&gt; geoid name pop_housed pop_renter median_rent #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 01 Alabama 4731852 1434765 747 #&gt; 2 13 Georgia 9943137 3592422 927 That looks a lot better! pivot_wider() threw out the error column and created one row for each state. "],["apis.html", "7 Introduction", " 7 Introduction An application programming interface (API) enables computers or computer programs to connect. In this part, we will explore what APIs are and how they can be used for tasks such as downloading data from, uploading data to, or manipulating data on another computer. "],["api-basics.html", "8 API basics 8.1 What is an API? 8.2 Find your API 8.3 Craft your request 8.4 Test your request 8.5 Read the data into R 8.6 Next steps", " 8 API basics library(tidyverse) APIs, especially if you’ve never worked with them before, can be confusing. In this chapter, we’ll try to demystify the process of working with APIs in R. You can use APIs for many different tasks, including reading in data, writing data (e.g., to Google Sheets), and carrying out tasks that you would usually carry out manually in your browser (e.g., pushing files to GitHub, moving files around in Google Drive, sending emails). We’ll focus just on reading data, but many of the ideas explained here are also relevant to other tasks. 8.1 What is an API? You’re already used to sending and receiving information over the internet. When you navigate to a URL in your browser, you’re requesting information. For example, if you open http://twitter.com, you’re requesting that Twitter sends the information for its homepage over to your computer. Then, when you type out a tweet and click Tweet, you send information back over to Twitter. APIs work in much the same way. APIs allow you to send and receive information over the internet, and many even use URLs to control what information is sent or received. The main difference between using the internet “normally” and using an API is that the “normal” internet is made for humans, while APIs are made for computers. The Twitter homepage is designed so that it’s easy for humans to read and send tweets, while the Twitter API is designed to make it easy for computers to download data and send tweets. You might think of using the internet normally like browsing a nicely lit, well-designed store, while using an API is like going directly to that store’s highly organized warehouse. The type of API we’re talking about here is actually called a web API. APIs (Application Programming Interfaces) are a much broader category that include all interfaces that facilitate communication between computer applications. However, when most people talk about APIs now, they mean web APIs. Next, we’ll walk through the general steps of using an API, with the Census Bureau’s API as an example. 8.2 Find your API For the rest of this chapter, we’ll use the United States Census Bureau as an example. The Census Bureau collects a large amount of data on the population of the United States and makes that data available through APIs. The first step to using an API is to determine if one exists for the task you want to accomplish, and where it’s documented. There are a couple ways to check if an organization supplies an API: Google Name of organization + API. This is usually pretty effective. Look for a Developers link on the organization homepage. This will often be at the bottom of the webpage, or under a Data tab. Like many organizations, the Census Bureau stores information about its APIs under a Developers page. You can navigate to this page from the Census Bureau homepage by going to Explore Data &gt; Developers. Organizations often assume that developers are the ones using APIs (but you don’t need to be a developer to use an API!). Once you’ve determined if an organization supplies APIs and where they document them, you need to determine which API you want. Many organizations will have multiple available APIs. Just like one organization (e.g., Google) can have multiple web applications (drive.google.com, gmail.com, calendar.google.com, etc.), organizations can have different APIs for different types of requests. The full list of Census Bureau APIs on its Available APIs page. Browse the available APIs to determine which one you want (these are sometimes listed under Endpoints or Resources instead). For our Census example, we’ll use the Vintage 2018 Population Estimates API, one of the many Population Estimates and Projections APIs. Vintage 2018 sounds like it refers to a bottle of wine, but actually refers to estimates between 2018 and the 2010 Census. Now that you’ve determined which API you to use, you’re ready to start constructing your API request. 8.3 Craft your request The specifics of an API request determine the information you’ll receive back. To figure out how to specify your request, you’ll typically need to consult your API’s documentation to determine two things: The base API request. This is analogous to a homepage URL, like https://www.wikipedia.org/. The API parameters. API parameters allow you to control what the request asks for, like how information at the end of a normal URL can point to a specific webpage (e.g., https://en.wikipedia.org/wiki/Tardigrade). 8.3.1 Base request For APIs whose requests are URLs, the base request will usually look something like api.[service-name].com/[other-details]. For the Census Bureau API we’re using, the base request is https://api.census.gov/data/2018/pep/population. 8.3.2 Parameters API parameters allow you to specify exactly what data you want. Figuring out API parameters is similar to figuring out function arguments. We need two pieces of information: What are the names of the parameters? How do we specify the values of the parameters? This is where the API documentation becomes especially important. Different organizations will document their APIs differently, but the documentation will typically include a table or list of the different parameters, as well as provide example requests. Let’s look at one of the examples from the Vintage 2018 API example page: https://api.census.gov/data/2018/pep/population?get=GEONAME,POP&amp;for=state:02&amp;key=YOUR_KEY_GOES_HERE The ? marks the end of the base API request and the beginning of the parameters section. The &amp;s separate the parameters from each other, just as commas separate function arguments in R. Finally, you supply values to parameters with =, just like in an R function call. The example request above has three parameters, all in lowercase: get, for, and key. We’ll talk more about the Census APIs in the next chapter. For now, just know that the example supplies the value GEONAME,POP to get and state:02 to for. key refers to an API access key. For some APIs, you’ll need to request a key through their website and then include that key in your request. Other APIs, as you’ll see in the Google Sheets chapter, require a more complex form of authentication. However, the Census API doesn’t require a key, so we’ll remove that parameter: https://api.census.gov/data/2018/pep/population?get=GEONAME,POP&amp;for=state:02 get specifies which variables from the data we’ll get back. The request says we want GEONAME (the name of the geography unit) and POP (population). for specifies the geographic unit. Here, we’ve said we want data for the state with the FIPS code 02, which is Alaska. Now that we have an API request, we can test it out! 8.4 Test your request Our API request is a URL, so we can open it in the browser. Click on our example API request to see what it gives you: https://api.census.gov/data/2018/pep/population?get=GEONAME,POP&amp;for=state:02 You should see: [[&quot;GEONAME&quot;,&quot;POP&quot;,&quot;state&quot;], [&quot;Alaska&quot;,&quot;737438&quot;,&quot;02&quot;]] Opening your request into the browser is a quick and easy way to check that you’ve gotten the request correct before reading the data into R. If you see a message saying “HTTP Status 404” or another error, your request probably has an error in it. In our example, our request was successful! We queried the Census Bureau’s API and got back the population of Alaska for 2018. 8.5 Read the data into R 8.5.1 JSON data If the result of your API request looks like what we got above, the API is sending you data in JSON (JavaScript Object Notation). JSON is a common format for data from APIs. XML is another common format. If the data is in XML, you’ll see a tag up at the top of the response that includes the word “xml.” Here, we need a way to read JSON into R. We can use fromJSON() from the jsonlite package. request &lt;- &quot;https://api.census.gov/data/2018/pep/population?get=GEONAME,POP&amp;for=state:02&quot; jsonlite::fromJSON(request) #&gt; [,1] [,2] [,3] #&gt; [1,] &quot;GEONAME&quot; &quot;POP&quot; &quot;state&quot; #&gt; [2,] &quot;Alaska&quot; &quot;737438&quot; &quot;02&quot; jsonlite::fromJSON() returns a matrix, not a tibble. We’ll need to do some extra work to get the data into a tidy tibble. request %&gt;% jsonlite::fromJSON() %&gt;% as_tibble() %&gt;% janitor::row_to_names(row_number = 1) #&gt; # A tibble: 1 × 3 #&gt; GEONAME POP state #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Alaska 737438 02 as_tibble() converts the matrix to a tibble, and janitor::row_to_names(row_number = 1) converts the first row to the column names. 8.5.2 All formats The httr package provides a more general way of getting data from an API. We get use httr::GET() to send our request to an API and capture the response. response &lt;- httr::GET(request) response #&gt; Response [https://api.census.gov/data/2018/pep/population?get=GEONAME,POP&amp;for=state:02] #&gt; Date: 2021-09-10 20:21 #&gt; Status: 200 #&gt; Content-Type: application/json;charset=utf-8 #&gt; Size: 53 B #&gt; [[&quot;GEONAME&quot;,&quot;POP&quot;,&quot;state&quot;], The result is a response object, and contains additional information about the result of your request. class(response) #&gt; [1] &quot;response&quot; httr contains functions for extracting information out of response objects. httr::http_error() returns TRUE if there was an error with your request, and FALSE otherwise. httr::http_error(response) #&gt; [1] FALSE We can also determine the type of data with httr::http_type(). httr::http_type(response) #&gt; [1] &quot;application/json&quot; It’s JSON, which matches what we discovered in the previous section. To extract out the actual data, we can use httr::content(). httr::content(response) #&gt; [[1]] #&gt; [[1]][[1]] #&gt; [1] &quot;GEONAME&quot; #&gt; #&gt; [[1]][[2]] #&gt; [1] &quot;POP&quot; #&gt; #&gt; [[1]][[3]] #&gt; [1] &quot;state&quot; #&gt; #&gt; #&gt; [[2]] #&gt; [[2]][[1]] #&gt; [1] &quot;Alaska&quot; #&gt; #&gt; [[2]][[2]] #&gt; [1] &quot;737438&quot; #&gt; #&gt; [[2]][[3]] #&gt; [1] &quot;02&quot; Now, the data is a list. To turn into a tibble, we’ll first turn it into a matrix. response %&gt;% httr::content() %&gt;% unlist() %&gt;% matrix(ncol = 3, byrow = TRUE) #&gt; [,1] [,2] [,3] #&gt; [1,] &quot;GEONAME&quot; &quot;POP&quot; &quot;state&quot; #&gt; [2,] &quot;Alaska&quot; &quot;737438&quot; &quot;02&quot; unlist() turns the content into an atomic vector, and matrix() turns the vector into a matrix. ncol specifies that we want three columns, and byrow = TRUE causes matrix() to fill the matrix row-by-row, instead of column-by-column. Then, we can apply the same steps as we did in the JSON section. response %&gt;% httr::content() %&gt;% unlist() %&gt;% matrix(ncol = 3, byrow = TRUE) %&gt;% as_tibble() %&gt;% janitor::row_to_names(row_number = 1) #&gt; # A tibble: 1 × 3 #&gt; GEONAME POP state #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Alaska 737438 02 For our Census Bureau case (and many other situations), jsonlite::fromJSON() is an easier solution. However, it’s useful to know about the httr package, especially if you’re working with an API in a script or package. httr::GET() and httr::http_error() allow you to programmatically handle errors, while jsonlite::fromJSON() will fail if something goes wrong. httr also contains many other functions for working with APIs—we’ve just covered the basics. In addition to using httr::GET() to read in data, you can use functions like httr::PUT() and httr::POST() to send data to APIs. You can read more about httr on the package website. 8.6 Next steps You now know the basics of working with APIs in R. Fortunately, there are R packages that wrap many of the commonly used APIs, hiding the messy details. If an R package is available, you won’t have to worry about creating an API request yourself or figuring out how to read in the resulting data. In the coming chapters, we’ll explain two such R packages: tidycensus and googlesheets4. For the APIs without a matching R package, however, it’s necessary to understand the workflow covered above. In the Census chapter, we’ll go over additional examples of this workflow. "],["census.html", "9 U.S. Census Bureau 9.1 Census Bureau basics 9.2 tidycensus 9.3 Population estimates 9.4 To learn more", " 9 U.S. Census Bureau library(tidyverse) library(tidycensus) 9.1 Census Bureau basics The U.S. Census Bureau is a fantastic resource for data related to the U.S. population. As you saw in the API Basics chapter, the Census Bureau makes a wide variety of APIs available. In this chapter, we’ll focus on three: the decennial census, the American Community Survey, and the population estimates. As we mentioned previously, many R packages wrap commonly used APIs, making it easier for you to obtain data. In this chapter, we’ll introduce one such package: tidycensus. We’ll show you how to use tidycensus to obtain data from the decennial census and American Community Survey. Then, we’ll go into more detail about working directly with the Census Bureau APIs. First, we’ll give a bit of background about three U.S. Census data sources: the decennial census, the American Community Survey, and the population estimates. 9.1.1 Decennial census When most people think of the U.S. Census, they’re thinking about the decennial census. The Census Bureau conducts the decennial census every ten years (starting in 1790), with the goal of determining the number of people living in the United States. Because many features of the U.S. government, including the number of representatives awarded to each state, depend on accurate population counts, the decennial census is required by the Constitution. For the decennial census, the Census Bureau tries to survey every household in the U.S. in an attempt to count every U.S. resident. The population estimates that come from the decennial census are therefore the most definitive that you can find. However, decennial census data come out only every ten years, so can be out-of-date. The decennial census survey also only asks a few questions, primarily about household size, race, ethnicity.1 The American Community Survey provides more detailed and up-to-date data. 9.1.2 ACS Between 1790 and 2000, decennial censuses included both a short form and a long form. Every household filled out the short form, but a sample also filled out the long form, which included additional questions. After 2000, the Census Bureau turned the long form into the American Community Survey (ACS), and began administering the ACS every year. The ACS, unlike the decennial census, is a sample. Every year, the ACS surveys a representative sample consisting of 3.5 million households. The Census Bureau then uses this sample to provide estimates for the entire U.S. population.2 The ACS calculates these estimates over two time periods: 1 year and 5 years. The 1-year estimates are the most current, but have larger margins of error due to their smaller sample size. Most of the time, you’ll use the 5-year estimates. Their larger sample sizes gives them greater accuracy, particularly for smaller geographic units. 9.1.3 Population estimates The third data source we’ll discuss comes from the Census Bureau’s Population Estimates Program (PEP). The decennial census publishes the definitive population of the United States every ten years. However, if you want to know the population of a U.S. geographic area between decennial census years, you’ll need to use the Population Estimates APIs. The ACS also includes population estimates, but the estimation techniques used for the Population Estimates Program are more accurate. 9.1.4 Choosing data The first step to working with U.S. Census data is to decide which data source to use. Here’s a quick guide: ACS Most of the time, you’ll want the ACS. The ACS includes many different variables on social, economic, housing, and demographic aspects. You’ll typically want the 5-year ACS, unless you’re looking at a large or rapidly changing geographic area or need yearly data. Decennial census Use the decennial census if you want definitive population data, don’t need that many variables, and don’t mind that the data is only available every 10 years. Population estimates Use data from the Population Estimates Program if you want accurate population data for a non-decennial census year (i.e., a year not divisible by 10). 9.2 tidycensus The tidycensus package wraps several U.S. Census Bureau APIs, allowing you to access decennial census and ACS data through R functions. Before you use tidycensus for the first time, you’ll need to obtain a Census Bureau API key. You can request one here. You’ll receive an email with your key. Copy your key to the clipboard, then navigate back to RStudio. Run the following line to open your .Renviron file: usethis::edit_r_environ() Then, add the following line, replacing YOUR_API_KEY with the key sent to you by the Census Bureau. CENSUS_API_KEY=YOUR_API_KEY Save and close the file, and then restart R (Ctrl/Cmd + Shift + F10) for the changes to take effect. From now on, you won’t need to worry about a key. Your key will stay in your .Renviron across R sessions. 9.2.1 Specify a dataset We’ll show you how to use the tidycensus package to access two Census APIs: the decennial census and the ACS. Before you start using tidycensus, you’ll need to decide which dataset and year to use. Dataset See our discussion in Choosing data for the trade-off between the decennial census, ACS, and the different ACS estimates. For many tidycensus functions, you specify the different surveys in the following way: \"acs5\": 5-year ACS \"acs1\": 1-year ACS \"sf1\": Decennial census sf stands for Summary File. Summary File 1 (\"sf1\") corresponds to the short form described earlier, while Summary File 3 (\"sf3\") corresponds to the long form. As explained earlier, the ACS took the place of the long form in 2001, so \"sf3\" is only available for censuses from 2000 or earlier. Year You’ll also need to decide on a year. For the decennial censuses, year will just be the year of the decennial census. Remember that the decennial census occurs in years ending in 0. The tidycensus package can access the 1990, 2000, and 2010 decennial censuses. For ACS data, the year argument of tidycensus functions refers to the end-year of the sample period. For example, if you want to use a 5-year ACS that ended in 2019, set year = 2019. As of July 2021, tidycensus supports 5-year ACS end-years 2009 through 2019, and 1-year ACS end-years 2005 through 2019. 9.2.2 Find variables Data from both the ACS and decennial census capture many variables. The 2010 decennial census includes 8,959, while the 2019 5-year ACS includes 27,040! A code, like H001001 or P011014, identifies each of these variables. To use tidycensus, you’ll need to determine the codes of your variables of interest. We’ll use the function tidycensus::load_variables() to find ACS or decennial census variables and their accompanying codes. load_variables() returns a tibble of all variable codes from a given dataset, alongside brief descriptions. We’ll load the variables from the 2019 5-year ACS as an example. all_vars_acs5 &lt;- load_variables(year = 2019, dataset = &quot;acs5&quot;) all_vars_acs5 #&gt; # A tibble: 27,040 × 3 #&gt; name label concept #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 B01001_001 Estimate!!Total: SEX BY AGE #&gt; 2 B01001_002 Estimate!!Total:!!Male: SEX BY AGE #&gt; 3 B01001_003 Estimate!!Total:!!Male:!!Under 5 years SEX BY AGE #&gt; 4 B01001_004 Estimate!!Total:!!Male:!!5 to 9 years SEX BY AGE #&gt; 5 B01001_005 Estimate!!Total:!!Male:!!10 to 14 years SEX BY AGE #&gt; 6 B01001_006 Estimate!!Total:!!Male:!!15 to 17 years SEX BY AGE #&gt; # … with 27,034 more rows load_variables() returns a tibble with the three variables: name: The variable code. label: A description of the variable. concept: A broader categorization. Let’s take a closer look at just one concept: sex by age. all_vars_acs5 %&gt;% filter(concept == &quot;SEX BY AGE&quot;) #&gt; # A tibble: 49 × 3 #&gt; name label concept #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 B01001_001 Estimate!!Total: SEX BY AGE #&gt; 2 B01001_002 Estimate!!Total:!!Male: SEX BY AGE #&gt; 3 B01001_003 Estimate!!Total:!!Male:!!Under 5 years SEX BY AGE #&gt; 4 B01001_004 Estimate!!Total:!!Male:!!5 to 9 years SEX BY AGE #&gt; 5 B01001_005 Estimate!!Total:!!Male:!!10 to 14 years SEX BY AGE #&gt; 6 B01001_006 Estimate!!Total:!!Male:!!15 to 17 years SEX BY AGE #&gt; # … with 43 more rows 49 variables belong to the “SEX BY AGE” concept. Each row refers to a variable under that concept. This way of thinking about variables can be a bit confusing at first. The Estimate!!Total variable captures the number of people for whom sex by age data is available. Estimate!!Total!!Male captures the total number of males, while Estimate!!Total!!Male!!Under 5 years captures the total number of males under 5 years old. This is a bit more intuitive for concepts like \"SEX BY AGE (ASIAN ALONE)\". all_vars_acs5 %&gt;% filter(concept == &quot;SEX BY AGE (ASIAN ALONE)&quot;) #&gt; # A tibble: 31 × 3 #&gt; name label concept #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 B01001D_001 Estimate!!Total: SEX BY AGE (ASIAN ALONE) #&gt; 2 B01001D_002 Estimate!!Total:!!Male: SEX BY AGE (ASIAN ALONE) #&gt; 3 B01001D_003 Estimate!!Total:!!Male:!!Under 5 years SEX BY AGE (ASIAN ALONE) #&gt; 4 B01001D_004 Estimate!!Total:!!Male:!!5 to 9 years SEX BY AGE (ASIAN ALONE) #&gt; 5 B01001D_005 Estimate!!Total:!!Male:!!10 to 14 years SEX BY AGE (ASIAN ALONE) #&gt; 6 B01001D_006 Estimate!!Total:!!Male:!!15 to 17 years SEX BY AGE (ASIAN ALONE) #&gt; # … with 25 more rows Here, Estimate!!Total represents the total number of Asian U.S. residents for whom sex/age data is relevant. Most concepts have a Estimate!!Total variable, or something similar. If you want to calculate a proportion, such as the proportion of males, use the relevant Estimate!!Total as the denominator. To find the variables you want, pipe the result of load_variables() into view(). all_vars_acs5 %&gt;% view() You can use the search bar to search for variables with a given a keyword (e.g., “income”). You can also click on the Filter button to get a search bar for each variable. Once you’ve found the variables you want, copy their codes (the name variable), and store them in a named vector. vars_acs5 &lt;- c( median_income = &quot;B06011_001&quot;, median_rent = &quot;B25064_001&quot; ) If we wanted to get variables from, say the 2010 decennial census, we’d use load_variables(year = 2010, dataset = &quot;sf1&quot;) %&gt;% view() The documentation for the ACS is helpful if you need additional information about ACS variables. 9.2.3 Get data tidycensus provides the functions get_acs() and get_decennial() to get Census Bureau ACS and decennial data. At minimum, you should supply these functions with three variables: geography variables year geography controls the geographic level of the data returned. The tidycensus website includes a helpful table of all available geographies. Common values are “state” and “county.” Supply variables with a vector of variable codes. Earlier, we stored the codes of some variables in the vector vars_acs5. vars_acs5 #&gt; median_income median_rent #&gt; &quot;B06011_001&quot; &quot;B25064_001&quot; For get_acs(), year indicates the end-year for the ACS estimates. If you want ACS estimates from 2015-2019, set year = 2019. By default, get_acs() uses the 5-year estimates. You can use other estimates by specifying survey. df_acs &lt;- get_acs( geography = &quot;state&quot;, variables = vars_acs5, year = 2019 ) #&gt; Getting data from the 2015-2019 5-year ACS df_acs #&gt; # A tibble: 104 × 5 #&gt; GEOID NAME variable estimate moe #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 01 Alabama median_income 26231 139 #&gt; 2 01 Alabama median_rent 792 5 #&gt; 3 02 Alaska median_income 34018 452 #&gt; 4 02 Alaska median_rent 1244 13 #&gt; 5 04 Arizona median_income 30216 88 #&gt; 6 04 Arizona median_rent 1052 4 #&gt; # … with 98 more rows get_acs() will return the estimate and margin of error (moe) for each variable. Because the ACS values are estimates, the Census Bureau calculates a margin of error for most variables. To pivot the data into a wider format, we can use pivot_wider(). df_acs %&gt;% pivot_wider( names_from = variable, values_from = c(estimate, moe) ) #&gt; # A tibble: 52 × 6 #&gt; GEOID NAME estimate_median… estimate_median… moe_median_inco… moe_median_rent #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 01 Alabama 26231 792 139 5 #&gt; 2 02 Alaska 34018 1244 452 13 #&gt; 3 04 Arizona 30216 1052 88 4 #&gt; 4 05 Arkansas 25758 745 139 4 #&gt; 5 06 California 31960 1503 65 4 #&gt; 6 08 Colorado 35887 1271 175 6 #&gt; # … with 46 more rows get_decennial() works similarly. First, we’ll find the variables with load_variables() and store several in a vector. load_variables(year = 2010, dataset = &quot;sf1&quot;) %&gt;% view() vars_decennial &lt;- c( pop_urban = &quot;H002002&quot;, pop_rural = &quot;H002005&quot; ) Then, we’ll use get_decennial() to access the data. This time, we’ll get the data at the county level. Here, year is the year of the decennial census. df_decennial &lt;- get_decennial( geography = &quot;county&quot;, variables = vars_decennial, year = 2010 ) #&gt; Getting data from the 2010 decennial Census #&gt; Using Census Summary File 1 df_decennial #&gt; # A tibble: 6,442 × 4 #&gt; GEOID NAME variable value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 05131 Sebastian County, Arkansas pop_urban 43495 #&gt; 2 05133 Sevier County, Arkansas pop_urban 2115 #&gt; 3 05135 Sharp County, Arkansas pop_urban 2177 #&gt; 4 05137 Stone County, Arkansas pop_urban 0 #&gt; 5 05139 Union County, Arkansas pop_urban 8830 #&gt; 6 05141 Van Buren County, Arkansas pop_urban 0 #&gt; # … with 6,436 more rows Again, we can use pivot_wider(). df_decennial %&gt;% pivot_wider(names_from = variable, values_from = value) #&gt; # A tibble: 3,221 × 4 #&gt; GEOID NAME pop_urban pop_rural #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 05131 Sebastian County, Arkansas 43495 11156 #&gt; 2 05133 Sevier County, Arkansas 2115 4772 #&gt; 3 05135 Sharp County, Arkansas 2177 7645 #&gt; 4 05137 Stone County, Arkansas 0 6712 #&gt; 5 05139 Union County, Arkansas 8830 10823 #&gt; 6 05141 Van Buren County, Arkansas 0 10345 #&gt; # … with 3,215 more rows Now, you can use your data however you wish. Note that, if you’re interested in geospatial aspects of ACS or decennial census data, we recommend using our ussf package for boundaries. You can install the package with the following command. remotes::install_github(&quot;dcl-docs/ussf&quot;) You can join your ACS or decennial census data with the result of ussf::boundaries(), using GEOID as a unique identifier. df_acs %&gt;% left_join( ussf::boundaries(geography = &quot;state&quot;) %&gt;% select(GEOID), by = &quot;GEOID&quot; ) #&gt; old-style crs object detected; please recreate object with a recent sf::st_crs() #&gt; # A tibble: 104 × 6 #&gt; GEOID NAME variable estimate moe geometry #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [m]&gt; #&gt; 1 01 Alabama median_income 26231 139 (((708460 -598743, 708114 -594376,… #&gt; 2 01 Alabama median_rent 792 5 (((708460 -598743, 708114 -594376,… #&gt; 3 02 Alaska median_income 34018 452 (((-1518019 -1148523, -1516049 -11… #&gt; 4 02 Alaska median_rent 1244 13 (((-1518019 -1148523, -1516049 -11… #&gt; 5 04 Arizona median_income 30216 88 (((-1743454 -375819, -1743828 -373… #&gt; 6 04 Arizona median_rent 1052 4 (((-1743454 -375819, -1743828 -373… #&gt; # … with 98 more rows You can make makes with this data using ggplot2 and geom_sf(). 9.3 Population estimates For population estimates outside decennial census years, you’ll need to work directly with the Population Estimates APIs. In the API Basics chapter, we discussed a workflow for working directly with APIs, and walked you through an example that used the Population Estimates. In this section, we’ll dive into more detail about the Population Estimates APIs. 9.3.1 Choose data There are two types of population estimate APIs: the vintages and the intercensals. Each vintage contains data on all years since the last decennial census. For example, the 2018 Vintage contains data for each year between 2010 and 2018. You should use the most recent vintage available, since the Census Bureau updates all previous years’ estimates. The intercensals contain data between previous decennial census. For example, the 2000-2010 intercensal contains yearly data for 2000 through 2010. If you want data from, for example, 2000 to 2018, you’ll need to use both a vintage and an intercensal, which will involve two API queries. As of February 2020, the Vintage 2018 estimates are the most recent, fully available estimates. Vintage 2019 estimates are available for some geographic units, but won’t be available at the county level until March 2020. Once you’ve decided on a vintage or a intercensal, click on the corresponding tab. You’ll see the various available APIs for those estimates. The links under the API name provide you with more information about the API. If you want population data, you’ll probably want the Population Estimates API, which provides yearly population estimates at various geographic levels. 9.3.2 Craft your request In the Census Basics chapter, we laid out the steps for working with an API. After finding your API, the next step is to craft the request. First, you’ll need to find the base request, which will be listed next to API Call. Next, add parameters to the base request to specify exactly what data you want. For the Population Estimates APIs, there are two important parameters: get and for. (You’ll also see key in the examples on the Census website, but an API key isn’t actually necessary.) get controls which variables (i.e., what will eventually become your tibble columns) the request returns. for controls the geographic level. The parameters come after a ? and are separated by a &amp;. We’ll go over both in more detail next. Variables To see all possible variables, click on the Variables link under your chosen API. This will lead you to a table of variables. The variables in all caps, like POP and GEONAME, are the names of variables returned by the API. The variables in lowercase, like for and in, are actually API request parameters. For now, just pay attention to the uppercase variables. You’ll specify the names of your desired variable after the API parameter get. Often, you’ll want to get population over time. To get data for each year, you’ll need the variables DATE_DESC and DATE_CODE. DATE_CODE is a code associated with a date (e.g., 1 or 2), and DATE_DESC describes the date (e.g., \"7/1/2018 population estimate\"). Without DATE_DESC, you won’t know what each DATE_CODE refers to, and without DATE_CODE, the API only returns data for one year. Geographies Next, you’ll need to specify what geographic level of the data. For each Population Estimate API, check which geographies are available by clicking on the Examples and Supported Geographies link. This link leads to a table with a description of the API, and links to examples, geographies, and other information. Click on the geographies link. Then, you’ll see a table with different geographies. Use these geographies to determine the data returned. For example: for=us will return data for the entire U.S. for=state:30 will return data for Montana. for=state:* will return data by state, for all states. for=county:* will return data by county, for all counties in the U.S. for=county:*&amp;in=state:30 will return data by county, just for Montana. 9.3.2.1 Examples Each API has an examples page. From the Population Estimates homepage, navigate to Examples and Supported Geographies &gt; Examples. This page lists examples for the 2018 Vintage Population Estimates API. Here are some more examples: Entire U.S. population data by year https://api.census.gov/data/2018/pep/population?get=GEONAME,DATE_CODE,DATE_DESC,POP&amp;for=us Population data for only Montana, by year https://api.census.gov/data/2018/pep/population?get=GEONAME,DATE_CODE,DATE_DESC,POP&amp;for=state:30 Population data for all counties, by year https://api.census.gov/data/2018/pep/population?get=GEONAME,DATE_CODE,DATE_DESC,POP&amp;for=county:* 9.3.3 Read the data into R The next step is to read your data into R. We’ll use the same process introduced in API Basics. Let’s use a request that gets population data for all states, by year. request &lt;- &quot;https://api.census.gov/data/2018/pep/population?get=GEONAME,DATE_CODE,DATE_DESC,POP&amp;for=state:*&quot; response &lt;- request %&gt;% jsonlite::fromJSON() %&gt;% as_tibble() %&gt;% janitor::row_to_names(row_number = 1) response #&gt; # A tibble: 572 × 5 #&gt; GEONAME DATE_CODE DATE_DESC POP state #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Alabama 1 4/1/2010 Census population 4779736 01 #&gt; 2 Alabama 2 4/1/2010 population estimates base 4780138 01 #&gt; 3 Alabama 3 7/1/2010 population estimate 4785448 01 #&gt; 4 Alabama 4 7/1/2011 population estimate 4798834 01 #&gt; 5 Alabama 5 7/1/2012 population estimate 4815564 01 #&gt; 6 Alabama 6 7/1/2013 population estimate 4830460 01 #&gt; # … with 566 more rows You’ll still need to do a bit of light cleaning. Also, notice that there are three different estimates for 2010 (the decennial census year). response %&gt;% distinct(DATE_DESC) %&gt;% pull(DATE_DESC) #&gt; [1] &quot;4/1/2010 Census population&quot; &quot;4/1/2010 population estimates base&quot; #&gt; [3] &quot;7/1/2010 population estimate&quot; &quot;7/1/2011 population estimate&quot; #&gt; [5] &quot;7/1/2012 population estimate&quot; &quot;7/1/2013 population estimate&quot; #&gt; [7] &quot;7/1/2014 population estimate&quot; &quot;7/1/2015 population estimate&quot; #&gt; [9] &quot;7/1/2016 population estimate&quot; &quot;7/1/2017 population estimate&quot; #&gt; [11] &quot;7/1/2018 population estimate&quot; You’ll generally want to use all the July 1st estimates so that all estimates are a year apart. 9.4 To learn more A good source to go deeper into working with census data is Walker, Analyzing US Census Data: Methods, Maps, and Models in R. U.S. Census Bureau. Questionnaires. https://www.census.gov/history/www/through_the_decades/questionnaires/↩︎ U.S. Census Bureau. American Community Survey: Information Guide. https://www.census.gov/content/dam/Census/programs-surveys/acs/about/ACS_Information_Guide.pdf↩︎ "],["googlesheets.html", "10 Google Sheets 10.1 Reading 10.2 Writing 10.3 Finding sheets 10.4 Authentication", " 10 Google Sheets library(tidyverse) library(googlesheets4) # Sheet ID for Gapminder example id_gapminder &lt;- &quot;1BzfL0kZUz1TsI5zxJF1WNF01IxvC67FbOJUiiGMZ_mQ&quot; Google Sheets are a useful way to collect, store, and collaboratively work with data. The googlesheets4 package wraps the Sheets API, making it easy for you to work with Google Sheets in R. The “4” in googlesheets4 refers to the most recent version (v4) of the Google Sheets API. There’s also an R package called googlesheets, which uses an older version (v3) of the Google Sheets API. If you’ve worked with the googlesheets package previously, note that the Sheets API v3 is being shut down, so you’ll need to switch over to googlesheets4. 10.1 Reading Reading data stored in a Google sheet into R will probably be your most common use of googlesheets4. Here, we’ll read in the data from our example sheet, which contains data from Gapminder. To read in the data, we need a way to identify the Google sheet. googlesheets4 supports multiple ways of identifying sheets, but we recommend using the sheet ID, as it’s stable and concise. You can find the ID of a Google sheet in its URL: If you want to extract an ID from a URL programmatically, you can also use the function as_sheets_id(). We’ve stored the ID for the Gapminder sheet in the parameters section up at the top. Here it is: id_gapminder #&gt; [1] &quot;1BzfL0kZUz1TsI5zxJF1WNF01IxvC67FbOJUiiGMZ_mQ&quot; Now, we can use the googlesheets4 function read_sheet() to read in the data. read_sheet()’s first argument, ss, takes the sheet ID. read_sheet(ss = id_gapminder) #&gt; ✓ Reading from &quot;test-gs-gapminder&quot;. #&gt; ✓ Range &#39;Africa&#39;. #&gt; # A tibble: 624 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Algeria Africa 1952 43.1 9279525 2449. #&gt; 2 Algeria Africa 1957 45.7 10270856 3014. #&gt; 3 Algeria Africa 1962 48.3 11000948 2551. #&gt; 4 Algeria Africa 1967 51.4 12760499 3247. #&gt; 5 Algeria Africa 1972 54.5 14760787 4183. #&gt; 6 Algeria Africa 1977 58.0 17152804 4910. #&gt; # … with 618 more rows Notice that the original sheet contains multiple sheets, one for each continent. We can list all these sheets by using the function sheet_names(). sheet_names(ss = id_gapminder) #&gt; [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; By default, read_sheet() reads in the first sheet. Here, that’s the Africa sheet. If we want to read in Asia, we can specify the sheet argument. read_sheet(ss = id_gapminder, sheet = &quot;Asia&quot;) #&gt; ✓ Reading from &quot;test-gs-gapminder&quot;. #&gt; ✓ Range &#39;&#39;Asia&#39;&#39;. #&gt; # A tibble: 396 × 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. #&gt; # … with 390 more rows 10.2 Writing You can write to a Google sheet using write_sheet(). If the sheet already exists, any data on it will be overwritten. 10.3 Finding sheets It can sometimes be difficult to find the exact Google Sheet you’re looking for. googlesheets4 includes a handy function that will return the names of the all your sheets, alongside their IDs, in an object called a dribble. A dribble is a tibble specifically for storing metadata about Google Drive files. all_my_sheets &lt;- gs4_find() all_my_sheets #&gt; # A dribble: 4 × 3 #&gt; name id drive_resource #&gt; &lt;chr&gt; &lt;drv_id&gt; &lt;list&gt; #&gt; 1 top-secret-data 1_vlFSg_2zP9JXo_tj-Ct5EGqCYpp0RC74DZ-7eEybUA &lt;named list [34]&gt; #&gt; 2 important-data 1rOnvdsEmqhTVjPcvFUpLftL8jmaPdSaJJo-_tBA6udk &lt;named list [34]&gt; #&gt; 3 my-sheet-2 1aRknFvsCDiYzwPkhv-g59fjPcojzJD71jyZM2H0j-t8 &lt;named list [34]&gt; #&gt; 4 my-sheet-1 1vSUFy9ENZXfcKFOb0woYuoBQj5Jgabm4b_1YD55dKGI &lt;named list [34]&gt; Note that gs4_find() will lists both sheets that you own and private sheets that you have access to. These are the same sheets that you can see on your Google Sheets homepage. Now, you can easily search for a sheet by piping the results of gs4_find() into view(). all_my_sheets %&gt;% view() 10.4 Authentication 10.4.1 Interactive session When you run R code in the console or in an R Markdown chunk, you’re in an interactive session. R understands that it’s interacting with a human, and so can prompt you for input or actions. In an interactive session, you don’t need to worry much about authentication. googlesheets4 will do most of the work for you. The first time you call a googlesheets4 function that requires authentication (e.g., read_sheet(ss = id_gapminder)), a browser tab will open and prompt you to sign into Google. Sign into your account and then return to RStudio. By default, your user credentials will now be stored as something called a gargle token. gargle is the name of an R package for working with Google APIs. The next time googlesheets4 requires authentication, it will use this token to authenticate you. 10.4.2 Non-interactive session When you knit an R Markdown, you’re using R non-interactively. googlesheets4 can’t prompt you to sign into Google, because it doesn’t assume that there’s a human standing by to do so. This should only be a problem if you’re trying to knit an R Markdown document that uses googlesheets4 and you’ve never authenticated with googlesheets4 before. The easiest way to quickly authenticate and set up your gargle token is to run googlesheets4::gs4_auth() (you can run this anywhere: console, R Markdown chunk, etc.). Once you’ve signed into Google and returned to RStudio, try knitting your document. If you’ve authenticated with googlesheets4 before, but your R Markdown document never finishing knitting, you may need to update your gargle token. Run googlesheets4::gs4_auth() and then try knitting again. "],["rvest.html", "11 rvest 11.1 Web page basics 11.2 Scrape data with rvest", " 11 rvest library(tidyverse) library(rvest) The rvest package (as in “harvest”) allows you to scrape information from a web page and read it into R. In this chapter, we’ll explain the basics of rvest and walk you through an example. 11.1 Web page basics 11.1.1 HTML HTML (Hyper Text Markup Language) defines the content and structure of a web page. In Chrome, you can view the HTML that generates a given web page by navigating to View &gt; Developer &gt; Developer tools. A series of elements, like paragraphs, headers, and tables, make up every HTML page. Here’s a very simple web page and the HTML that generates it. The words surrounded by &lt; &gt; are HTML tags. Tags define where an element starts and ends. Elements, like paragraph (&lt;p&gt;), headings (&lt;h1&gt;), and tables (&lt;table&gt;), start with an opening tag (&lt;tagname&gt;) and end with the corresponding closing tag (&lt;/tagname&gt;). Elements can be nested inside other elements. For example, notice that the &lt;tr&gt; tags, which generate rows of a table, are nested inside the &lt;table&gt; tag, and the &lt;td&gt; tags, which define the cells, are nested inside &lt;tr&gt; tags. The HTML contains all the information we’d need if we wanted to read the animal data into R, but we’ll need rvest to extract the table and turn it into a data frame. 11.1.2 CSS CSS (Cascading Style Sheets) defines the appearance of HTML elements. CSS selectors are often used to style particular subsets of elements, but you can also use them to extract elements from a web page. CSS selectors often reflect the structure of the web page. For example, the CSS selector for the example page’s heading is body &gt; h1 and the selector for the entire table is body &gt; table You don’t need to generate CSS selectors yourself. In the next section, we’ll show you how to use your browser to figure out the correct selector. 11.2 Scrape data with rvest Our World in Data compiled data on world famines and made it available in a table. Using this table as an example, we’ll show you how to use rvest to scrape a web page’s HTML, read in a particular element, and then convert HTML to a data frame. 11.2.1 Read HTML First, copy the url of the web page and store it in a parameter. url_data &lt;- &quot;https://ourworldindata.org/famines&quot; Next, use rvest::read_html() to read all of the HTML into R. url_data %&gt;% read_html() #&gt; {html_document} #&gt; &lt;html&gt; #&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... #&gt; [2] &lt;body class=&quot;&quot;&gt;\\n&lt;header class=&quot;site-header&quot;&gt;&lt;div class=&quot;wrapper site-nav ... read_html() reads in all the HTML for the page. The page contains far more information than we need, so next we’ll extract just the famines data table. 11.2.2 Find the CSS selector We’ll find the CSS selector of the famines table and then use that selector to extract the data. In Chrome, right click on a cell near the top of the table, then click Inspect (or Inspect element in Safari or Firefox). The developer console will open and highlight the HTML element corresponding to the cell you clicked. Hovering over different HTML elements in the Elements pane will highlight different parts of the web page. Move your mouse up the HTML document, hovering over different lines until the entire table (and only the table) is highlighted. This will often be a line with a &lt;table&gt; tag. Right click on the line, then click Copy &gt; Copy selector (Firefox: Copy &gt; CSS selector; Safari: Copy &gt; Selector Path). Return to RStudio, create a variable for your CSS selector, and paste in the selector you copied. css_selector &lt;- &quot;#tablepress-73&quot; 11.2.3 Extract the table You already saw how to read HTML into R with rvest::read_html(). Next, use rvest::html_element() to select just the element identified by your CSS selector. url_data %&gt;% read_html() %&gt;% html_element(css = css_selector) #&gt; {html_node} #&gt; &lt;table id=&quot;tablepress-73&quot; class=&quot;tablepress tablepress-id-73&quot;&gt; #&gt; [1] &lt;thead&gt;&lt;tr class=&quot;row-1 odd&quot;&gt;\\n&lt;th class=&quot;column-1&quot;&gt;Year&lt;/th&gt;\\n&lt;th class= ... #&gt; [2] &lt;tbody class=&quot;row-hover&quot;&gt;\\n&lt;tr class=&quot;row-2 even&quot;&gt;\\n&lt;td class=&quot;column-1&quot;&gt; ... The data is still in HTML. Use rvest::html_table() to turn the output into a tibble. url_data %&gt;% read_html() %&gt;% html_element(css = css_selector) %&gt;% html_table() #&gt; # A tibble: 77 × 6 #&gt; Year Country `Excess Mortalit… `Excess Mortali… `Excess Mortali… Source #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1846–52 Ireland 1,000,000 1,000,000 1,000,000 Ó Grád… #&gt; 2 1860-1 India 2,000,000 2,000,000 2,000,000 Kumar … #&gt; 3 1863-67 Cape Verde 30,000 30,000 30,000 Ó Grád… #&gt; 4 1866-7 India 961,043 961,043 961,043 Kumar … #&gt; 5 1868 Finland 100,000 100,000 100,000 Ó Grád… #&gt; 6 1868-70 India 1,500,000 1,500,000 1,500,000 Imperi… #&gt; # … with 71 more rows Now, the data is ready for wrangling in R. Note that html_table() will only work if the HTML element you’ve supplied is a table. If, for example, we wanted to extract a paragraph of text, we’d use html_text() instead. css_selector_paragraph &lt;- &quot;body &gt; main &gt; article &gt; div.content-wrapper &gt; div.offset-content &gt; div &gt; div &gt; section:nth-child(1) &gt; div &gt; div:nth-child(1) &gt; p:nth-child(9)&quot; url_data %&gt;% read_html() %&gt;% html_element(css = css_selector_paragraph) %&gt;% html_text() #&gt; [1] &quot;The entry is based on a global dataset of famines since the mid-19th century produced by us. This ‘Our World in Data-Dataset of Famines’ can be found at the very end of this document and is preceded by a discussion of how this dataset was constructed and on which sources it is based.&quot; "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
